{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_model_summary import summary\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "from network.dsunetr import DS_UNETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DS-UNETR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DS_UNETR(in_channels=1,\n",
    "                out_channels=8,\n",
    "                img_size=[64, 128, 128],\n",
    "                feature_size=16,\n",
    "                num_heads=[4,4,4,4],\n",
    "                depths=[3, 3, 3, 3],\n",
    "                dims=[32, 64, 128, 256],\n",
    "                do_ds=False,\n",
    "                ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "           Layer (type)                                                                                Output Shape         Param #     Tr. Param #\n",
      "====================================================================================================================================================\n",
      "       SpatialEncoder-1     [1, 64, 256], [1, 32, 32, 32, 32], [1, 64, 16, 16, 16], [1, 128, 8, 8, 8], [1, 64, 256]       6,683,296       6,683,296\n",
      "       ChannelEncoder-2     [1, 64, 256], [1, 32, 32, 32, 32], [1, 64, 16, 16, 16], [1, 128, 8, 8, 8], [1, 64, 256]       7,671,856       7,671,856\n",
      "         UnetResBlock-3                                                                       [1, 16, 64, 128, 128]           7,360           7,360\n",
      "           ConcatConv-4                                                                         [1, 32, 32, 32, 32]         111,904         111,904\n",
      "           ConcatConv-5                                                                         [1, 64, 16, 16, 16]         447,040         447,040\n",
      "           ConcatConv-6                                                                           [1, 128, 8, 8, 8]       1,787,008       1,787,008\n",
      "           ConcatConv-7                                                                           [1, 256, 4, 4, 4]       7,145,728       7,145,728\n",
      "    BIFPN_Fusion_Conv-8              [1, 32, 32, 32, 32], [1, 64, 16, 16, 16], [1, 128, 8, 8, 8], [1, 256, 4, 4, 4]       1,060,622       1,060,622\n",
      "    BIFPN_Fusion_Conv-9              [1, 32, 32, 32, 32], [1, 64, 16, 16, 16], [1, 128, 8, 8, 8], [1, 256, 4, 4, 4]       1,060,622       1,060,622\n",
      "   BIFPN_Fusion_Conv-10              [1, 32, 32, 32, 32], [1, 64, 16, 16, 16], [1, 128, 8, 8, 8], [1, 256, 4, 4, 4]       1,060,622       1,060,622\n",
      "        UnetrUpBlock-11                                                                           [1, 128, 8, 8, 8]       1,460,008       1,460,008\n",
      "        UnetrUpBlock-12                                                                         [1, 64, 16, 16, 16]         373,672         373,672\n",
      "        UnetrUpBlock-13                                                                         [1, 32, 32, 32, 32]         100,840         100,840\n",
      "        UnetrUpBlock-14                                                                       [1, 16, 64, 128, 128]          30,208          30,208\n",
      "        UnetOutBlock-15                                                                        [1, 8, 64, 128, 128]             136             136\n",
      "====================================================================================================================================================\n",
      "Total params: 29,000,922\n",
      "Trainable params: 29,000,922\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "input: torch.Size([1, 1, 64, 128, 128])\n",
      "output: torch.Size([1, 8, 64, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# forward\n",
    "x = torch.ones([1, 1, 64, 128, 128]).cuda()\n",
    "y = model(x)\n",
    "\n",
    "# summary\n",
    "print(summary(model, x))\n",
    "print('input:', x.shape)\n",
    "print('output:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::mul encountered 543 time(s)\n",
      "Unsupported operator aten::fill_ encountered 486 time(s)\n",
      "Unsupported operator aten::sub encountered 18 time(s)\n",
      "Unsupported operator aten::ne encountered 18 time(s)\n",
      "Unsupported operator aten::clone encountered 42 time(s)\n",
      "Unsupported operator aten::add encountered 226 time(s)\n",
      "Unsupported operator aten::softmax encountered 54 time(s)\n",
      "Unsupported operator aten::gelu encountered 54 time(s)\n",
      "Unsupported operator aten::mul_ encountered 13 time(s)\n",
      "Unsupported operator aten::leaky_relu_ encountered 20 time(s)\n",
      "Unsupported operator aten::add_ encountered 44 time(s)\n",
      "Unsupported operator aten::sum encountered 18 time(s)\n",
      "Unsupported operator aten::div encountered 18 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "concat_conv1.out_proj2, concat_conv2.out_proj2, concat_conv3.out_proj2, concat_conv4.out_proj2, fusion.0.fusion6.upsample, fusion.1.fusion6.upsample, fusion.2.fusion6.upsample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 29.0 M\n",
      "MAdds: 44.99 G\n"
     ]
    }
   ],
   "source": [
    "# n_parameters & flops\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "flops = FlopCountAnalysis(model, x)\n",
    "model_flops = flops.total()\n",
    "\n",
    "print(f\"Total trainable parameters: {round(n_parameters * 1e-6, 2)} M\")\n",
    "print(f\"MAdds: {round(model_flops * 1e-9, 2)} G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of DS_UNETR(\n",
      "  (spatial_encoder): SpatialEncoder(\n",
      "    (downsample_layers): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Convolution(\n",
      "          (conv): Conv3d(1, 32, kernel_size=(2, 4, 4), stride=(2, 4, 4), bias=False)\n",
      "        )\n",
      "        (1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): PatchMerging3D(\n",
      "          (reduction): Linear(in_features=256, out_features=64, bias=False)\n",
      "        )\n",
      "        (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): PatchMerging3D(\n",
      "          (reduction): Linear(in_features=512, out_features=128, bias=False)\n",
      "        )\n",
      "        (1): GroupNorm(64, 128, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): PatchMerging3D(\n",
      "          (reduction): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): GroupNorm(128, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (stages): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (channel_encoder): ChannelEncoder(\n",
      "    (downsample_layers): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Convolution(\n",
      "          (conv): Conv3d(1, 32, kernel_size=(2, 4, 4), stride=(2, 4, 4), bias=False)\n",
      "        )\n",
      "        (1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): PatchMerging3D(\n",
      "          (reduction): Linear(in_features=256, out_features=64, bias=False)\n",
      "        )\n",
      "        (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): PatchMerging3D(\n",
      "          (reduction): Linear(in_features=512, out_features=128, bias=False)\n",
      "        )\n",
      "        (1): GroupNorm(64, 128, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): PatchMerging3D(\n",
      "          (reduction): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): GroupNorm(128, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (stages): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): ChannelTransformerBlock(\n",
      "          (ctf): ChannelTransformer(\n",
      "            (cnlblock): ChannelAttnBlock(\n",
      "              (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): ChannelAttn(\n",
      "                (qkv): Linear(in_features=32, out_features=96, bias=False)\n",
      "                (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "                (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): ChannelTransformerBlock(\n",
      "          (ctf): ChannelTransformer(\n",
      "            (cnlblock): ChannelAttnBlock(\n",
      "              (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): ChannelAttn(\n",
      "                (qkv): Linear(in_features=32, out_features=96, bias=False)\n",
      "                (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "                (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): ChannelTransformerBlock(\n",
      "          (ctf): ChannelTransformer(\n",
      "            (cnlblock): ChannelAttnBlock(\n",
      "              (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): ChannelAttn(\n",
      "                (qkv): Linear(in_features=32, out_features=96, bias=False)\n",
      "                (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "                (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): ChannelTransformerBlock(\n",
      "          (ctf): ChannelTransformer(\n",
      "            (cnlblock): ChannelAttnBlock(\n",
      "              (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): ChannelAttn(\n",
      "                (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "                (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "                (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): ChannelTransformerBlock(\n",
      "          (ctf): ChannelTransformer(\n",
      "            (cnlblock): ChannelAttnBlock(\n",
      "              (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): ChannelAttn(\n",
      "                (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "                (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "                (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): ChannelTransformerBlock(\n",
      "          (ctf): ChannelTransformer(\n",
      "            (cnlblock): ChannelAttnBlock(\n",
      "              (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): ChannelAttn(\n",
      "                (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "                (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "                (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): ChannelTransformerBlock(\n",
      "          (ctf): ChannelTransformer(\n",
      "            (cnlblock): ChannelAttnBlock(\n",
      "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): ChannelAttn(\n",
      "                (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
      "                (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): ChannelTransformerBlock(\n",
      "          (ctf): ChannelTransformer(\n",
      "            (cnlblock): ChannelAttnBlock(\n",
      "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): ChannelAttn(\n",
      "                (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
      "                (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): ChannelTransformerBlock(\n",
      "          (ctf): ChannelTransformer(\n",
      "            (cnlblock): ChannelAttnBlock(\n",
      "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): ChannelAttn(\n",
      "                (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
      "                (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): ChannelTransformerBlock(\n",
      "          (ctf): ChannelTransformer(\n",
      "            (cnlblock): ChannelAttnBlock(\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): ChannelAttn(\n",
      "                (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "                (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): ChannelTransformerBlock(\n",
      "          (ctf): ChannelTransformer(\n",
      "            (cnlblock): ChannelAttnBlock(\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): ChannelAttn(\n",
      "                (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "                (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): ChannelTransformerBlock(\n",
      "          (ctf): ChannelTransformer(\n",
      "            (cnlblock): ChannelAttnBlock(\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): ChannelAttn(\n",
      "                (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "                (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder1): UnetResBlock(\n",
      "    (conv1): Convolution(\n",
      "      (conv): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    )\n",
      "    (conv2): Convolution(\n",
      "      (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    )\n",
      "    (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (norm1): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (norm2): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (conv3): Convolution(\n",
      "      (conv): Conv3d(1, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    )\n",
      "    (norm3): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      "  (decoder4): UnetrUpBlock(\n",
      "    (upsampling): PatchExpanding3D(\n",
      "      (expand): Linear(in_features=256, out_features=1024, bias=False)\n",
      "    )\n",
      "    (decoder_block): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder3): UnetrUpBlock(\n",
      "    (upsampling): PatchExpanding3D(\n",
      "      (expand): Linear(in_features=128, out_features=512, bias=False)\n",
      "    )\n",
      "    (decoder_block): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder2): UnetrUpBlock(\n",
      "    (upsampling): PatchExpanding3D(\n",
      "      (expand): Linear(in_features=64, out_features=256, bias=False)\n",
      "    )\n",
      "    (decoder_block): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SpatialTransformerBlock(\n",
      "          (stf): SpatialTransformer(\n",
      "            (swindual): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.15, inplace=False)\n",
      "                  (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder1): UnetrUpBlock(\n",
      "    (upsampling): Convolution(\n",
      "      (conv): ConvTranspose3d(32, 16, kernel_size=(2, 4, 4), stride=(2, 4, 4), bias=False)\n",
      "    )\n",
      "    (decoder_block): ModuleList(\n",
      "      (0): UnetResBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (norm2): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fusion): ModuleList(\n",
      "    (0): BIFPN_Fusion_Conv(\n",
      "      (fusion1): MFC_two(\n",
      "        (w1_relu): ReLU()\n",
      "        (upsample): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (conv): DepthwiseConvBlock(\n",
      "          (depthwise): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=128, bias=False)\n",
      "          (pointwise): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn): BatchNorm3d(128, eps=4e-05, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (fusion2): MFC_two(\n",
      "        (w1_relu): ReLU()\n",
      "        (upsample): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (conv): DepthwiseConvBlock(\n",
      "          (depthwise): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=64, bias=False)\n",
      "          (pointwise): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn): BatchNorm3d(64, eps=4e-05, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (fusion3): MFC_two(\n",
      "        (w1_relu): ReLU()\n",
      "        (upsample): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (conv): DepthwiseConvBlock(\n",
      "          (depthwise): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=32, bias=False)\n",
      "          (pointwise): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn): BatchNorm3d(32, eps=4e-05, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (fusion4): MFC_three(\n",
      "        (conv): DepthwiseConvBlock(\n",
      "          (depthwise): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=64, bias=False)\n",
      "          (pointwise): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn): BatchNorm3d(64, eps=4e-05, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "        (w2_relu): ReLU()\n",
      "        (downsample): Conv3d(32, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "      )\n",
      "      (fusion5): MFC_three(\n",
      "        (conv): DepthwiseConvBlock(\n",
      "          (depthwise): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=128, bias=False)\n",
      "          (pointwise): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn): BatchNorm3d(128, eps=4e-05, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "        (w2_relu): ReLU()\n",
      "        (downsample): Conv3d(64, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "      )\n",
      "      (fusion6): MFC_two(\n",
      "        (w1_relu): ReLU()\n",
      "        (upsample): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (downsample): Conv3d(128, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (conv): DepthwiseConvBlock(\n",
      "          (depthwise): Conv3d(256, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=256, bias=False)\n",
      "          (pointwise): Conv3d(256, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn): BatchNorm3d(256, eps=4e-05, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): BIFPN_Fusion_Conv(\n",
      "      (fusion1): MFC_two(\n",
      "        (w1_relu): ReLU()\n",
      "        (upsample): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (conv): DepthwiseConvBlock(\n",
      "          (depthwise): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=128, bias=False)\n",
      "          (pointwise): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn): BatchNorm3d(128, eps=4e-05, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (fusion2): MFC_two(\n",
      "        (w1_relu): ReLU()\n",
      "        (upsample): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (conv): DepthwiseConvBlock(\n",
      "          (depthwise): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=64, bias=False)\n",
      "          (pointwise): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn): BatchNorm3d(64, eps=4e-05, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (fusion3): MFC_two(\n",
      "        (w1_relu): ReLU()\n",
      "        (upsample): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (conv): DepthwiseConvBlock(\n",
      "          (depthwise): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=32, bias=False)\n",
      "          (pointwise): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn): BatchNorm3d(32, eps=4e-05, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (fusion4): MFC_three(\n",
      "        (conv): DepthwiseConvBlock(\n",
      "          (depthwise): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=64, bias=False)\n",
      "          (pointwise): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn): BatchNorm3d(64, eps=4e-05, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "        (w2_relu): ReLU()\n",
      "        (downsample): Conv3d(32, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "      )\n",
      "      (fusion5): MFC_three(\n",
      "        (conv): DepthwiseConvBlock(\n",
      "          (depthwise): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=128, bias=False)\n",
      "          (pointwise): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn): BatchNorm3d(128, eps=4e-05, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "        (w2_relu): ReLU()\n",
      "        (downsample): Conv3d(64, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "      )\n",
      "      (fusion6): MFC_two(\n",
      "        (w1_relu): ReLU()\n",
      "        (upsample): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (downsample): Conv3d(128, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (conv): DepthwiseConvBlock(\n",
      "          (depthwise): Conv3d(256, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=256, bias=False)\n",
      "          (pointwise): Conv3d(256, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn): BatchNorm3d(256, eps=4e-05, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): BIFPN_Fusion_Conv(\n",
      "      (fusion1): MFC_two(\n",
      "        (w1_relu): ReLU()\n",
      "        (upsample): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (conv): DepthwiseConvBlock(\n",
      "          (depthwise): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=128, bias=False)\n",
      "          (pointwise): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn): BatchNorm3d(128, eps=4e-05, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (fusion2): MFC_two(\n",
      "        (w1_relu): ReLU()\n",
      "        (upsample): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (conv): DepthwiseConvBlock(\n",
      "          (depthwise): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=64, bias=False)\n",
      "          (pointwise): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn): BatchNorm3d(64, eps=4e-05, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (fusion3): MFC_two(\n",
      "        (w1_relu): ReLU()\n",
      "        (upsample): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (conv): DepthwiseConvBlock(\n",
      "          (depthwise): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=32, bias=False)\n",
      "          (pointwise): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn): BatchNorm3d(32, eps=4e-05, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (fusion4): MFC_three(\n",
      "        (conv): DepthwiseConvBlock(\n",
      "          (depthwise): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=64, bias=False)\n",
      "          (pointwise): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn): BatchNorm3d(64, eps=4e-05, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "        (w2_relu): ReLU()\n",
      "        (downsample): Conv3d(32, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "      )\n",
      "      (fusion5): MFC_three(\n",
      "        (conv): DepthwiseConvBlock(\n",
      "          (depthwise): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=128, bias=False)\n",
      "          (pointwise): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn): BatchNorm3d(128, eps=4e-05, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "        (w2_relu): ReLU()\n",
      "        (downsample): Conv3d(64, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "      )\n",
      "      (fusion6): MFC_two(\n",
      "        (w1_relu): ReLU()\n",
      "        (upsample): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (downsample): Conv3d(128, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (conv): DepthwiseConvBlock(\n",
      "          (depthwise): Conv3d(256, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=256, bias=False)\n",
      "          (pointwise): Conv3d(256, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn): BatchNorm3d(256, eps=4e-05, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (concat_conv1): ConcatConv(\n",
      "    (out_proj): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (out_proj2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (conv1): UnetResBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv2): UnetResBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (concat_conv2): ConcatConv(\n",
      "    (out_proj): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (out_proj2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (conv1): UnetResBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv2): UnetResBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (concat_conv3): ConcatConv(\n",
      "    (out_proj): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (out_proj2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (conv1): UnetResBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv2): UnetResBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (concat_conv4): ConcatConv(\n",
      "    (out_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (out_proj2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (conv1): UnetResBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (norm2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv2): UnetResBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (norm2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (out1): UnetOutBlock(\n",
      "    (conv): Convolution(\n",
      "      (conv): Conv3d(16, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify training feasibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients computed successfully.\n",
      "Model parameters updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy input tensor\n",
    "dummy_input = torch.randn(1, 1, 64, 128, 128).to(device)  # Batch size of 1, in_channels of 1, and image size of 64x128x128\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Perform a forward pass\n",
    "output = model(dummy_input)\n",
    "\n",
    "# Create a dummy target tensor with the same shape as the output\n",
    "dummy_target = torch.randn_like(output)\n",
    "\n",
    "# Define a loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Compute the loss\n",
    "loss = criterion(output, dummy_target)\n",
    "\n",
    "# Perform a backward pass to check if gradients can be computed\n",
    "try:\n",
    "    loss.backward()\n",
    "    print(\"Gradients computed successfully.\")\n",
    "\n",
    "    # Update the model parameters\n",
    "    optimizer.step()\n",
    "    print(\"Model parameters updated successfully.\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dvaa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
